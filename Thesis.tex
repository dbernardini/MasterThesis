	% !TeX encoding = UTF-8
% !TeX program = pdflatex
\documentclass[binding=0.6cm,LaM]{sapthesis}
\usepackage[utf8]{inputenx}
\usepackage{hyperref}
\hypersetup{pdftitle={Thesis},pdfauthor={Danilo Bernardini}}
\title{Design, development and evaluation of a framework for recording and synchronizing experiences in VR with physiological signals}
\author{Danilo Bernardini}
\IDnumber{1544247}
\course{Engineering in Computer Science}
\courseorganizer{Facolt√† di Ingegneria dell'informazione, informatica e statistica}
\AcademicYear{2017/2018}
\copyyear{2018}
\advisor{Prof. Massimo Mecella}
%\coadvisor{Prof. Maurizio Caon}
\authoremail{danilo.bernardini93@gmail.com}
\begin{document}
\frontmatter
\maketitle
\tableofcontents
\mainmatter
\chapter{Introduction}

\chapter{Technologies}

This chapter introduces the involved technologies - virtual reality and physiological sensors -  and presents the current state of the art about them, including the available devices on the market and some application examples.

\section{Virtual Reality}
Virtual Reality (VR) is a technology that aims to allow a user to experience a computer-generated simulated environment. It commonly consists of a visual experience, but it can also include audio, haptic, touch and other feedback devices. 

Visual devices are usually VR headsets, head-mounted displays (HMD) with two screens - one per eye - that immerse the user in a virtual world simulating the real one. While wearing the headset, the user can look around rotating his head, move in the environment or interact with virtual objects using controllers, and perceive other senses through ad-hoc devices.

The above systems are usually computer-based, i.e. they are physically connected to the computer, so the applications run here and the visual output is transmitted to the headset display.
Another kind of solution is the one that uses the smartphone: in this case the headset is just a case that holds the device, the whole computation takes place in the smartphone and thus the applications are much simpler and less realistic compared to the computer-based ones (see \ref{sec:vrheadsets}).
	
\subsection{VR spread}
Virtual reality idea is not something new, there have been basic examples of it since the 1960s. The first head-mounted display system was the \textit{Sword of Damocles} \cite{sutherland1968head} created in 1968 by computer scientist Ivan Sutherland and his student Bob Sproull: the graphics were very primitive and consisted only of wireframe rooms, but the device was able to change the showed perspective according to the user head position. Its name comes from its appearance: the whole system was too heavy to be worn by a person so it was attached to the ceiling, suspended on the user's head.

In the next years different VR systems were developed, most of the times for the videogames industry. Examples can be found in some \textit{SEGA} and \textit{Nintendo} products. 

The main reason why VR is exploded in the last few years is that technology is now powerful enough to support advanced computation and to give users really immersive experiences. Until a few years ago VR solutions could not satisfy \textit{immersion} requirements (see section \ref{sec:measures}): graphics were not good enough and they had low refresh rates, causing the user some sickness (see section \ref{sec:measures}).

Conversely, in the present days we can rely on powerful machines and tools that allow us to create and experience good simulations of the world, most of the times without perceiving any issue.

VR spread is also encouraged by the release of not very expensive headsets (see section \ref{sec:vrheadsets}): a lot of people can now buy VR systems, either for development or entertainment, but also companies are starting to use them for business (see next section).

We can refer to \textit{Gartner Hype Cycle for Emerging Technologies} of 2017 \cite{hypecycle} to understand VR position in the market. Gartner is an American research and advisory company that provides IT-related insights and market analysis. The Hype Cycle is "a graphical depiction of a common pattern that arises with each new technology or other innovation" \cite{hypecycledef}, i.e. a chart that shows the maturity and social application of a technology. The plot is divided in five phases representing the stages of a technology life cycle: Innovation Trigger, Peak of Inflated Expectations, Trough of Disillusionment, Slope of Enlightenment, Plateau of Productivity. 
Gartner releases it every year and in 2017 they stated that transparently immersive experiences are one of the 3 most trending topics, together with AI everywhere and digital platforms, and that VR is currently in the fourth phase of the cycle. This means that VR applications are starting to be understood and adopted by companies, that methodologies and best practices are being developed and that the technology is beginning to spread to the potential audience.


\subsection{Applications}
Virtual Reality has many applications in different fields, from gaming and entertainment to education and healthcare. Here are some examples of industries that are using VR for improving and enhancing their work.

\begin{description}

\item[Business:] Companies can make clients experience virtual tours of business environments; they can test and show new products before releasing them or train their employees using VR.

\item[Culture and education:] VR can be used in museums and historical settings to recreate ancient sites, monuments, cities; it can be
very useful for teaching, since students can be immersed in what they are studying and better understand things (e.g. history, geography, astronomy).

\item[Games and entertainment:] Gaming and entertaining industry is probably the biggest adopter of VR, since this brings the player in a whole new level of immersion and realism; VR is also used in movies, sport and arts.

\item[Healthcare:] VR allows to simulate a human body, so that students and doctors can study and train on it; it can simulate a surgery or perform a robotic one, i.e. control a robotic arm remotely; it can be used to treat phobias and diseases.

\item[Military:] VR can be used to perform a combat simulation where soldiers can train and learn battlefield tactics; it can be also useful for flight simulation or medical training on the battlefield.  

\item[Science and engineering:] Virtual reality technology can help scientists to visualize and interact with complex structures, or it can be used by engineers to design, model and see something in 3D. 

\end{description}


\subsection{Measures}
\label{sec:measures}
The previous sections mentioned the words \textit{immersion}, \textit{realism}, \textit{sickness}, etc. These are concepts we can analyze and measure during VR experiences. 

\subsubsection{Immersion and presence}
There is a lot of confusion about the concepts of immersion and presence; some people think they are the same thing, some others mix them up. According to Mel Slater, immersion concerns \textit{what the technology delivers from an objective point of view. The more that a system delivers displays (in all sensory modalities) and tracking that preserves fidelity in relation to their equivalent real-world sensory modalities, the more that it is 'immersive'} \cite{slater2003note}. 
This means that immersion is something that can be determined and somehow measured in an objective way, depending entirely on the technology and not on the user. 
Presence, on the contrary, is a user-dependent concept, something that varies depending on the person, a human reaction to immersion: each person can perceive a different level of presence inside the same system and even a single user can perceive a different level of presence using the same system in different times, depending on the user's emotional state, past history, etc. \cite{bowman2007virtual}.
We can say that presence is both immersion and involvement: this regards the state of focus and attention of the user and depends on the interaction with the virtual world, the storyline, and other similar features of the experience.

\paragraph{Measuring immersion}
Since immersion is an objective concept depending only on technology, we can measure it evaluating how close the system video, audio and other features are to the real world \cite{bowman2007virtual}. For example if we consider visual source, we can identify some parameters that influence it: 
\begin{itemize}
\item field of view and field of regard, respectively the size of the visual field that can be viewed instantaneously and the total size of the visual field surrounding the user;
\item display size and resolution;
\item stereoscopy and head-based rendering, given by head tracking;
\item frame rate and refresh rate.
\end{itemize}

\paragraph{Measuring presence}
Presence is a subjective measure and is not easy to quantify. During the years some techniques have been developed and tested, the most used ones are the following \cite{sanchez2005presence}:

\begin{description}

\item [Questionnaires]
Users participate to the virtual experience and then answer a questionnaire about presence. The questions imply responses between two extremes (e.g. from "no presence" to "complete presence"). The problem with this approach is that asking questions about presence can affect the actual perception of the participant.

\item [Behavioral]
We can see evidence of presence if users in the virtual world behave as if they were in the real world. This can be triggered by events that cause a physical reaction on the participant, such as a movement reflex or a body rotation.

\item [Physiological signals]
This is a specialization of the previous approach: if we know how a person physiologically reacts to an event, and we can find the same reaction during a virtual event, then this is a sign of presence. This technique can only be used when we have well-known and easy to measure reactions, for example fear, so it is not ideal for calm and "boring" scenarios where nothing happens.

\item [Breaks in presence (BIP)]
A break in presence is an event that occurs when the user becomes aware he is in a virtual environment. This can happen if the visual stimuli starts to lag, if the graphics become low-quality, if the user touches a physical object from outside the VR experience, etc. This approach allows to know presence by analyzing the moments when BIP occur, and it is a good alternative to the physiological one because it can be used in every kind of environment (calm, stressfull,scary, and so on).

\end{description}

\subsubsection{Co-presence}
When more than one participant share the same virtual environment we can measure co-presence (also known as shared presence). It is the feeling that the other participants are really present and that the user is interacting with real people \cite{casanueva2001effects}. As well as presence, also co-presence is measured with questionnaires. It is not proven that co-presence is related to presence, because some studies \cite{tromp1998small, slater2000small} seem to find correlation between them and others \cite{casanueva2001effects} do not.

\subsubsection{VR sickness}
As anticipated, virtual reality can cause some sort of sickness to the user. The common symptoms are similar to motion sickness symptoms: general discomfort, nausea, sweating, headache, disorientation, fatigue \cite{cobb1999virtual}.
Sickness varies from person to person and it is usually caused by conflicts between perceived movement and actual movement: if the player walks in VR the eyes say he is walking while the ears do not detect any movement, creating confusion to the brain. Other aspects that can induce sickness are low refresh rate and poor animations: both these things have the same effect on brain, which processes frames at higher rates or expects better animations.

Since it is a subjective feeling, the most common way of measuring VR sickness is through questionnaires. The standard methodology for measuring sickness is the \textit{Simulator Sickness Questionnaire (SSQ)} by Kennedy, Lane, Berbaum and Lilienthal \cite{kennedy1993simulator}. 
An interesting alternative approach is to monitor the postural activity of the participant \cite{riccio1991ecological}: it seems that motion sickness is related to postural stability and that there are differences in postural activity between people who are experiencing sickness and people who are not.

\subsubsection{Situation awareness}
A general measure that applies also for VR is situation awareness. It consists in having the awareness of what is happening in the surrounding environment and what may happen in the future, being ready to handle the situation that will arise. A famous approach to measure it is the \textit{Situational Awareness Rating Technique (SART)} \cite{selcon1990evaluation} originally developed by Taylor and Selcon in 1990 for evaluating pilots. It is a post-trial questionnaire that asks the user to rate 10 dimensions with a number from 1 to 7.

\subsubsection{Workload}
Another measure that can be useful in VR is workload, meant as the effort needed to complete a task. The most used technique is the
\textit{NASA Task Load Index (NASA-TLX)} \cite{hart1988development}, a questionnaire divided in two parts: the first one consists of 6 rating subscales, the second one is a personal weighting of these subscales.


\subsection{VR headsets}
\label{sec:vrheadsets}	
This section presents the most important available VR devices, starting from the computer-connected (tethered) headsets and continuing with mobile ones. Finally, standalone VR devices are introduced. 

\subsubsection{Tethered VR headsets}
Computer-connected VR systems take advantage of the computing power of the machine they are connected to, so they can give the user complex environments and experiences. The headsets provide head tracking and motion tracking - generally through external base stations - so the user can move with 6 degrees of freedom (DOF) and can interact with the virtual world in a lot of possible ways. 

\begin{description}

\item[HTC Vive]
HTC Vive system consists of a headset, two controllers and two base stations to track body movements. The display has 90 Hz refresh rate and 110 degree field of view, with a resolution of 1080x1200 per eye. The base stations also track the controllers, allowing an advanced interaction with the virtual environment. Together with several sensors, the headset also includes a front-facing camera. In 2018 HTC launched the Pro version of the Vive, fitted with a higher-resolution display, attachable headphones and a second camera.

\item[Oculus Rift]
Oculus initiated a Kickstarter campaign in 2012 and in 2014 it was acquired by Facebook. The system includes two Touch controllers and the headset hardware is basically the same as the HTC Vive's one. Tracking is obtained with \textit{Constellation}, an optical-based tracking system that detects IR LED markers on the HMD and controllers.

\item[Sony PlayStation VR]
PlayStation VR is Sony's proprietary VR system only compatible with PlayStation 4. It supports only PS4 ad-hoc games but there is the possibility to play any other PS4 game like if it was on a very large screen. Headset display has a resolution of 960x1080 per eye, a 100 degrees field of view and a native refresh rate of 90 or 120 Hz. Regarding the controller input, the system supports both regular \textit{DualShock 4} or \textit{PlayStation Move} controllers. Players need a \textit{PlayStation Camera} to track the HMD and the Move controllers.

\end{description}

\subsubsection{Mobile VR headsets}

A mobile VR system consists of a case that holds the smartphone and two lens that separate the display in two parts, one per eye. Since the only available sensors are those included in the smartphone, these VR systems can not track body movements and therefore they can just count on 3 DOF (head rotation). Since they are generally simpler and less powerful than the tethered ones, mobile VR systems are usually quite cheaper.

\begin{description}

\item[Google Daydream View]
Daydream is the enhanced successor of the \textit{Google Cardboard}, a very basic cardboard-made headset with two lenses that turns the smartphone into a VR system. Daydream software is built into the Android operating system since the Nougat version. The package comes with a wireless touchpad controller that can be tracked with on-board sensors.

\item[Samsung Gear VR]
Gear VR is a system only compatible with some high-level Samsung devices, it contains a controller equipped with a touchpad and a motion sensor. The development was carried out by Samsung in collaboration with Oculus, which took care of the software distribution. 

\end{description}

\subsubsection{Standalone VR headsets}
Standalone headsets are a new type of VR systems that does not require a computer or a smartphone in order to work. Since they have almost the same technical specifications, these devices computing power can be compared to that of smartphones, even if they are optimized for VR applications.

\begin{description}

\item[Oculus Go]
This device was developed by Oculus in collaboration with Qualcomm and Xiaomi. The HMD has 3 degree of freedom and is equipped with a 5.5-inch display with a resolution of 1280x1440 per eye, a Snapdragon 821 processor and comes with a 32GB or 64GB storage. The system also includes a wireless controller. 

\item[Lenovo Mirage Solo]
Mirage Solo is a brand new device and it works with Google Daydream platform. Technically speaking, its display is similar to the Oculus Go's one but the device is powered by a Snapdragon 835 with a 4GB RAM and it has a better tracking system, that gives the headset 6 DOF (but only 3 for the controller).

\end{description}


\subsection{Tracking devices}	
Together with headsets, a lot of different sensors and devices can be used to enhance VR experience. The most significant improvement in interaction is probably tracking. Most HMDs have an integrated head tracking system that allows 6 DOF movement, and controllers are usually tracked making it possible to interact with the environment. However, there exist external systems that enable advanced tracking functionalities such as full body tracking, hand tracking or eye tracking.

\subsubsection{Body tracking}
Body tracking makes possible to follow movements of the whole body, included arms and legs. It is a technique often used in movies and videogames to animate digital characters in CGI (in this case it is known as \textit{motion capture}). There are several companies providing body tracking solutions, here follows a list of some of the most valuable products currently available.

\begin{description}

\item[HoloSuit]
It is a suit that allows full body motion tracking and capturing thanks to the many sensors it has embedded. It also provides haptic feedback and it is compatible with the most important operating systems and development environments (\textit{Unity}, \textit{Unreal Engine}).

\item[HTC Vive Trackers]
Trackers are small disc-shaped devices that work with HTC Vive and they can be attached to any real physical object in order to track its movements. They can be also connected to the user to obtain body tracking or to replace Vive controllers.

\item[Optitrack]
Optitrack is one of the largest motion capture companies in the world, providing powerful tools and devices able to track small markers from long distances.

\item[Perception Neurons]
This system is composed of small units called Neurons that are placed to the various parts of the body, for example to fully track a person 11 to 32 Neurons are required. It is compatible with most 3D modeling and animation programs.

\item[PrioVR]
PrioVR uses sensors attached to key points of the body to provide full tracking without the need of a camera. It comes with small controllers and it works with both Unity and Unreal Engine.

\item[Orbbec]
Orbbec produces long-rage depth cameras that can be used with the proprietary body tracking SDK to fully detect the human body. It works with Windows, Linux and Android.

\item[Senso Suit]
Senso Suit is a kit composed of 15 modules, each containing a sensor and a vibrating motor, that make it possible to achieve full body tracking.  SDK available for Unity, Unreal Engine, C++ and Android. 

\item[VicoVR]
VicoVR is a wireless device that provides full body tracking to mobile VR headsets without the need of body-attached sensors. The device look is similar to \textit{Microsoft Kinect} (discontinued) and is compatible with Android, iOS and the main mobile VR headsets.

\item[Xsens]
Another big name in tracking industry, Xsens provides a variety of solutions for full body tracking, including suits or strap-based sensors kits. It does not need a camera and works with almost every 3D animation program.

\end{description}

\subsubsection{Hand tracking}
Hand tracking is a complex aspect of artificial intelligence that allows to detect and track hand movements only with a camera, without the need of any controller or external device. Here are presented two devices and a software library capable of detecting hand movements.

\begin{description}

\item[LeapMotion]
LeapMotion tracking system is a camera that needs to be attached to a VR headset in order to track hands with a field of view of 135 degrees. It works with both HTC Vive and Oculus Rift.

\item[uSens Fingo]
Fingo consists of a camera similar to LeapMotion that works with both tethered and mobile VR headsets, but it is optimized for mobile. Currently it is only compatible with Unity.

\item[OpenCV]
OpenCV (Open source Computer Vision) is a software library that provides several computer vision features, including hand tracking. It is written in C++ but there exist wrappers for other programming languages.

\end{description}

\subsubsection{Eye tracking}
Eye tracking is the process of detecting eye position and gaze. It is usually implemented with lights and cameras that analyze eye movement and detect the direction of gaze. Eye tracking is used in a lot of different fields such as safety (for example in a car), advertising, marketing and psychology.

\begin{description}

\item[aGlass]
aGlass is an eye tracking module compatible with HTC Vive. It is composed of two lenses that are placed above the Vive lenses and provide low-latency eye tracking with a field of view of 110 degrees.

\item[Fove]
Fove is a VR headset with an incorporated eye tracking module. It is compatible with SteamVR and OSVR and supports both Unity and Unreal Engine.

\item[Pupil]
Pupil Labs produces binocular eye tracking add-ons for the leading VR headsets. It comes with open source software and it works with Unity. 

\item[Tobii]
Tobii proposes an hardware eye tracking development kit for HTC Vive or a retrofitted version of the Vive with an eye tracking integration. 

\end{description}

\section{Physiological signals}
Physiological parameters are vital signals that describe a person's functions and activities state. There exist many different physiological signals, measurable with a lot of different sensors and devices. This section presents a wide variety of physiological sensors, from cheap wearable devices to expensive professional kits.

\subsection{Wearable devices}

\begin{description}

\item[Empatica E4 Wristband]
Wearable band and app for visualization and analysis, it comes with mobile API and Android SDK. 

Sensors: photoplethysmogram (PPG), accelerometer, EDA, thermometer.

\item[EQ02 LifeMonitor]
Device that can be worn on the chest and that stores or transmits the data to a mobile phone or a computer.

Sensors: ECG, respiration, skin temperature, accelerometer.

\item[Helo LX]
Smartband and app, it is compatible with Windows, Android, iOS.

Sensors: heart rate, breath rate, blood pressure.

\item[Microsoft Band 2]
Smartband by Microsoft, it can also work with Cortana. The app is available for Android, iOS and Windows Phone.

Sensors: heart rate

\item[Polar H10]
Heart rate sensor to attach on the chest, it comes with an internal memory and it also works with third-party applications. 

Sensors: heart rate

\item[Xiaomi Mi Band 2]
Band mainly used to monitor fitness exercises and to track sleep activity. 

Sensors: accelerometer, heart rate

\end{description}

\subsection{Professional kits}

\begin{description}

\item[Biopack Systems]
Hardware and software platform that provides professional measurements and analysis for research and education. The main module can acquire up to 16 different channels but it has no internal memory and it needs an external power supply.

\item[BiosignalsPlux]
BiosignalsPlux makes research kits that include 4 or 8 sensors and a 4 or 8 channels wireless hub, which has internal memory and battery. It works with third parties applications and sensors and comes with  Android, C++, Python, Java and MATLAB APIs.

\item[Libelium MySignals]
Software (uses an includes box screen) or hardware (uses Arduino) versions, acquired data is sent to the cloud and can be visualized on the web or on the mobile app. Up to 18 different sensors can be connected to the system. Available SDK for the hardware version.

\item[MindMedia NeXus-10]
Wireless acquisition system that supports 8 different signals, it includes a software for visualization and synchronization, together with a SD card and a battery for ambulant recordings.

\item[Thought Technology Biofeedback System]
5 or 8 channels system, it comes with proprietary software for data visualization and includes a memory card for offline acquisitions.

\end{description}



%\section{Usability}



\chapter{Project concept}
This chapter introduces the project idea and the concepts behind it, starting from the use of VR combined with physiological signals and going towards use cases and project requirements.

\section{VR and physiological signals}
Virtual reality is a powerful tool not only for commercial use, but also for research. Since the beginning of its lifetime, in fact, people started to think that it could have been a good environment to test and study certain phenomenons, especially concerning human body and behavior. From the first VR basic prototypes to the advanced systems of these last years, researchers have conducted lost of experiments about people reactions and emotions with VR making use of physiological signals. The reason why virtual reality is so appropriate and useful for research is that it can simulate real situations, so it gives researchers countless opportunities: they can recreate real world environments, test new kinds of interaction, make people relive specific scenes, and so on. 

As mentioned above, for many years there have been many examples of studies about VR with the use of physiological signals. An early research paper dealing with this subject was the one about fear of flying written by Wiederhold and others and published in 1998 \cite{wiederhold1998fear}. The authors wanted to test if there are significant physiological differences between people who are afraid of flying and people who are not. In order to accomplish that, they studied the body responses of two groups of people, one suffering from a fear of flying and the other one not, while experiencing a virtual reality flight. They measured heart rate, peripheral skin temperature, respiration rate, sweat gland activity and brain wave activity. The results of the experiment were successful and they could conclude that there exist significant differences between these two groups of people. In addition, the authors also succeeded in reducing arousal of people with fear of flying making them experience the simulation for several times. 

The following year another research paper about VR and physiological signals was published by Cobb and others \cite{cobb1999virtual}. This was one of the first investigations about VR sickness (see \ref{sec:measures}): the authors examined effects and symptoms of virtual reality on people using several methodologies, including physiological parameters.

These examples are part of the several research studies conducted to analyze or treat specific phobias and diseases. More recent ones are, for example, those by Kuriakose and Lahiri \cite{kuriakose2015understanding} and by Seinfeld and others \cite{seinfeld2016influence}, published in 2015 and 2016, respectively. Both are about anxiety: the former aims at measuring anxiety level in adolescents with autism using physiological signals while being in VR, the latter examines the influence of music on anxiety induced by fear of heights in VR. 

Finally, we can include in this list a paper about the hand illusion in VR with temperature monitoring by Llobera and others \cite{llobera2013relationship} and one about reduction of pain and anxiety in dental procedures using VR distractions \cite{wiederhold2014clinical}.

\section{Idea and requirements}
There are many experiments and studies using virtual reality that also include physiological signals; the previous section presented some of them showing that VR applications are limitless and that using physiology with it can lead to important results.
There is a common aspect in all these kind of studies: if the researchers want to collect physiological data during a virtual reality experience, they need to set up everything. This means to find a way to follow the experiment, to see what the participant sees in VR, to store data and to be able to analyze it. All these things are possible only if the collected data - VR video, physiological signals and other available sources - are synchronized. This process implies a lot of effort from the researchers because they have to figure out how to design and realize it, wasting time they could invest in the actual research.

This leads to the basic motivation behind this thesis project: designing and implementing a framework for recording and synchronizing experiences in VR with physiological signals. The framework would be a useful tool for whoever wants to physiologically analyze a certain phenomenon in VR. Having a ready-to-use test environment would allow to save time and focus on the actual experiment, since the only required thing is to build the virtual scene and everything else is done by the framework.

\subsection{Use cases}
The project requirements come from use cases. We can imagine two main cases that cover the two different parts of the application usage:

\begin{enumerate}

\item examiner wants to see in real-time and to record what the participant sees and how the body behaves;

\item examiner wants to replay a previously recorded session.

\end{enumerate}

Let us analyze these points one by one in order to fully understand what the system should do. After a brief description, for each use case is presented a diagram followed by the list of steps required to arrive to the goal, including the extensions.

\begin{enumerate}

\item Since the framework is designed for conducting VR researches with physiological signals, who conducts the experiment should be able to see what the participant sees in the virtual world in real-time and how the body reacts to the experience. 
In addition to this, the examiner should be able to record the whole session including all the signals and streams he has. This leads to an extension of the case, named \textit{Record session}, which can be executed only after the examiner is able to see everything correctly.
Once recorded, everything can be synchronized in order to be easily accessible and analyzable in the future. This brings the \textit{Record session} case to be extend in turn. 

The use case description assumes that the participant is ready, with all the physiological sensors attached to his body and the VR headset correctly placed.

\begin{figure}[h]
\centering
\includegraphics[scale=.24]{images/use_case_1}
\caption{\textit{Real-time monitor} case diagram}
\end{figure}

\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{| l | l |}
  \hline			
  Name & Real-time monitor \\
  \hline
  Initiator & Examiner \\
  \hline
  Goal & Monitor the session in real-time \\
  \hline  
\end{tabular}

\vspace{.5cm}

\renewcommand{\arraystretch}{1}
\begin{tabular}{ l }
\textbf{Main success scenario} \\
1. Examiner runs the application\\
2. Examiner sets up the sensors in the application \\
3. Application displays sensors, VR and camera streams \\
4. Examiner sees everything in real-time \\

\vspace{0.1cm}\\

\textbf{Extensions} \\
4. Examiner wants to record the session\\
\quad a. Examiner starts a new recording \\
\quad b. VR, camera and sensors streams are stored \\
\quad c. Examiner stops the recording \\

4c. Examiner wants to synchronize the session\\
\quad a. Examiner starts the synchronization \\
\quad b. Synchronization completes \\

\end{tabular}

\end{center}

\item The examiner should have the possibility to offline replay a previously recorded session, being able to see everything as it was during the real-time monitoring. In this replay mode there should be the possibility to find key moments in which some relevant event happened. Since the session sources are synchronized, this task should be easy. The examiner should be able to tag these events placing markers on them: these would allow a simple navigation on the session and would also be very useful for analysis.

\begin{figure}[h]
\centering
\includegraphics[scale=.257]{images/use_case_2}
\caption{\textit{Record session} case diagram}
\end{figure}

\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{| l | l |}
  \hline			
  Name & Replay session \\
  \hline
  Initiator & Examiner \\
  \hline
  Goal & Replay a prerecorded session \\
  \hline  
\end{tabular}

\vspace{.5cm}

\renewcommand{\arraystretch}{1}
\begin{tabular}{ l }
\textbf{Main success scenario} \\
1. Examiner runs the application\\
2. Examiner selects the "replay" mode\\
3. Examiner opens one of the directories containing a previous recorded session \\
4. Examiner plays the session \\

\vspace{0.1cm}\\

\textbf{Extensions} \\
4. Examiner wants to insert one or more marker\\
\quad a. Examiner finds the moments he wants to mark \\
\quad b. Examiner inserts the marker and assigns it a label \\
\end{tabular}

\end{center}


\end{enumerate}

The whole UML Use Case Diagram is represented in figure 3.3. 

\begin{figure}[h]
\centering
\includegraphics[scale=.257]{images/use_case}
\caption{Global Use Case Diagram}
\end{figure}


\subsection{Requirements}
This section is about the requirements for the project, which result from the above use cases and from some other considerations. 

The system should allow to monitor a virtual reality experience while some sensors collect the participant's physiological signals. It would be reasonable to include in the system a camera that films the participant during the experience. This would make it possible to also have an external view on the user, being able to monitor his physical reactions to events. Examiners can see if the participant moves, rotates his body, is not stable, or has any other kind of reaction in response to a virtual stimulus.

Moreover, system structure and usability should not be hard, considering that the potential users will not be engineers or computer science experts. These people, in fact, will reasonably be scientist, doctors or researchers that need to test and analyze something in VR. 

The system should allow to to start a new real-time monitoring session which shows what the participant sees in VR, the camera source filming him and his physiological signals. During this "live" session, the examiner should be able to start a new recording: this will store the streams from VR, camera and sensors. At the end of the experiment the examiner should stop the recording and the various data need to be automatically synchronized. Alongside with the real-time mode, the system should have an "offline" part that allows to replay a previously recorded session. This mode should provide classical playback controls such as play, pause and stop, as well as timeline navigation to quickly skip to a specific frame. Since everything is already synchronized, in this modality the examiner can easily track every moment of the experiment; if there are relevant events it is worth to emphasize, the system should allow to place markers on them. 
Finally, the stored files (VR video, camera video and sensors data) should remain independent from each other, i.e. they should still be accessible from external programs after the synchronization and markers insertion.


\chapter{Design}
This chapter describes the design of the system, starting from the initial idea and continuing with the multiple steps until the final scheme.

\section{Similar systems}
Before thinking about the design itself, a research on similar projects was conducted in order to better understand how this kind of systems are realized. The list includes not only projects that are close to this one as a whole, but also projects that provide just some similar features. Here follows the research result, divided in the various fields regarding the functionalities of the system. 

\begin{description}

\item [PhysioVR] \cite{munoz2016physiovr} is a framework developed to integrate physiological signals in mobile VR applications. Signals are acquired using wearable devices connected to the smartphone via Bluetooth and include heart rate, electroencephalography (EEG) and electromyography (EMG). The system allows to record data and to communicate with the VR content allowing an external event triggering using a real-time control.

\item [MuLES] \cite{cassani2015mules} is a tool for acquiring and streaming EEG signals that aims at facilitate the development of brain-computer interface programs. The system allows to replay a session and there are clients developed for use on different platforms and in various programming languages.

\item [PhysSigTK] \cite{rank2015physsigtk} is a toolkit for accessing low-cost physiological signals hardware from Unity. It aims at helping developers to exploit engagement in their effective games. 

\item [RehabNet] \cite{vourvopoulos2013rehabnet} is a system that helps patients rehabilitation through serious videogames that monitor physiological parameters. It supports a large variety of external physiological sensors. 

\item [Bicycle4CaRe] \cite{baldassini2017customization} is a tool for domestic physical training that uses VR and sensors to monitor user's activity. It simulates a home environment and collects data both from it and from the user, providing real-time feedback.

\item [Cube] \cite{cepisca2015platform} is a VR simulation platform for the supervision of personnel working in critical infrastructures. It collects physiological data during the virtual experience to identify the optimal physiological profile of personnel for using it in real scenarios.

\item [Athena] \cite{mcgregor2017integrating} is an analytics platform that acquires data from a combat simulator, combining this data with physiological signals of the player, and sends him body feedback.

\item [VAST] \cite{kuriakose2015understanding} is a virtual reality system for social communication that also collects physiological signals to determine the user's anxiety level.

\item [VR-SAAFE] \cite{bekele2017design} is a VR-based system developed for understanding facial emotions in subjects suffering of schizophrenia. The system uses an eye tracker and physiological signals to monitor user's eye gaze and emotions.

\end{description}

\subsection{Design}
The above systems use different approaches for what concerns design. 

\begin{itemize}

\item \textit{PhysioVR} is composed of two layers: the first one deals with synchronizing the devices and streaming the data, the second one is a Unity package that receives and analyzes data and physiological signals. There are also other two modules for external control and data recording.

\item \textit{MuLES} acts as the server part of a client-server architecture. It handles the communication between the several EEG sensors and transmits their data to the client applications. The devices are connected with their own drivers and the connection with the clients is realized through TCP/IP.

\item \textit{RehabNet} architecture is based on three building blocks: an hardware layer that deals with devices connection, a control panel that filters and cleans data, and a web interface for accessing the rehabilitation tools. The whole system is organized as a client-server application.

\item \textit{Bicycle4CaRe} is composed of different biofeedback devices connected to the PC via an Arduino board,ca desktop application and a VR simulated environment. The app contains the main functionalities and allows to control the experience.

\item \textit{Athena} is consists of a data acquisition layer that collects the signals from the sensors and the game data from the combat simulator. This layer sends data to IBM's Infosphere Streams, that processes it and generates analytics for storage.

\item \textit{VAST} is divided in three modules. The first one presents the tasks, the second one adapts and changes tasks according to the users behavior and the last one is the data acquisition layer. 

\item \textit{VR-SAAFE} is divided in three components too: task presentation environment based on Unity, eye tracking application and the physiological signals acquisition module.

\end{itemize}

\subsection{Physiological sensors}
This section presents the physiological sensors used by similar projects. This could help understanding which are the most used and useful signals in this kind of applications.
\textit{PhysioVR} uses only three signals: 

\begin{itemize}

\item heart rate, acquired through Android wearable devices such as smartbands, smartwatches or camera-based sensors;

\item EEG, obtained from a low-cost -wearable headband called Muse BCI;

\item EMG, acquired with Myo Armband, a device that contains 8 sensors and that recognizes gestures.

\end{itemize}
\textit{PhysSigTK} uses sensors considering criteria such as ease of use, price, software support and low-level access to data. Therefore the employed devices are four, measuring several different signals:  

\begin{itemize}

\item Empatica E3/E4, that measures EDA, blood volume, temperature and movement;

\item Wild Divine Iom, that provides skin conductance level (SCL) and heart rate;

\item e-Healt, an Arduino-based device that supports different sensors. In this case heart rate, galvanic skin response (a.k.a. EDA), ECG and breathing activity;  

\item NeuroSky Mindwave, a low-cost EEG sensor that also provides a heart rate sensor for the ear.

\end{itemize}
For what concerns \textit{RehabNet}, it uses protocols such as the \textit{Virtual-Reality Peripheral Network (VRPN)} and \textit{Open Sound Control (OSC)} for integrating a large variety of external devices and sensors (trackers, haptic devices, analog input, sound, etc). In addition to this, it natively supports three sensors:

\begin{itemize}

\item EEG acquired with Emotiv EPOC, a wireless EEG headset;

\item EMG, collected through Myomo mPower 1000, a prosthetic arm that also integrates sensors;

\item kinematic data, acquired with the Microsoft Kinect. 

\end{itemize}
\textit{VR-SAAFE} monitors physiological signals and eye gaze:

\begin{itemize}

\item Biopac Bionomadix measures PPG (from which heart rate can be extracted), skin temperature, EMG, respiration and galvanic skin response;

\item Tobii X120 for eye tracking. 

\end{itemize}
\textit{Bicycle4CaRe} measures both user's parameters and environmental conditions. The latter are not relevant for the project of this thesis; the former are:

\begin{itemize}

\item heart rate acquired with a pulse-oximeter sensor;

\item breath rate measured through a respiration monitor belt;

\item blood pressure collected with a sphygmomanometer. 

\end{itemize}

\subsection{Connectivity and libraries}
The project application will need to connect to the devices and to acquire signals. In order to be able to do it, similar systems techniques and methodologies were explored. Here are reported some examples of connectivity and external libraries approaches. 

\begin{itemize}

\item \textit{PhysioVR} transmits data over the UDP protocol. In this way the communication can be multicast and the transmission of all the physiological signals is done using a single port. Data can be accessed even from external devices with UDP capabilities. Users can access from remote and interact with the system bidirectionally, thus receiving data and sending feedback. The application also provides client scripts for different programming languages such as Java, C\#, Python, MATLAB.

\item \textit{MuLES} acquires data from the hardware using third party APIs, SDKs, drivers or implementations of the specific device communication protocol. On the other hand, transmission to the client application is realized through the TCP/IP protocol, which allows to send data to a local network or over the Internet.
This means that client scripts can be written in any programming language supporting basic socket programming, such as Java or Python.

\item \textit{RehabNet} sends data to a smartphone running the app using a custom implementation of the UDP protocol. It also connects to an analysis and tracking tool. As anticipated above, the system makes use of VRPN and OSC protocols in order to connect to other external devices or libraries, for example the OpenViBE BCI software.

\item The interesting thing about \textit{Cube} is that it provides different connectivity solutions that can be used simultaneously or not. An example of it is ECG acquisition: it can be wired or wireless (Bluetooth) transmitted to a local machine, transmitted to a smartphone over the Internet or using a satellite connection

\end{itemize}

\subsection{Events and triggers}
Since the system should deal with key events and markers, here are presented the solutions adopted by \textit{PhysioVR} and \textit{Bicycle4Care}.

\begin{itemize}

\item \textit{PhysioVR} acquires data from the UDP connection and makes it available inside the Unity environment. This brings developers to have physiological data usable inside the experience, allowing some game parameters to be bound to physiological signals, or events to be triggered when certain conditions are verified. Another way of creating events is to have external inputs: the VR user can receive triggers influencing the experience scenario from an external application that monitors in real-time the person's vitals.

\item \textit{Bicycle4Care} reads data in real-time and guides the user through the experience with ad-hoc messages. Like for PhysioVR, events can be triggered when some parameter exceed a certain threshold. 

\end{itemize}

\subsection{Synchronization}
After the acquisition, data needs to be synchronized. Among the presented systems, only \textit{Athena} and \textit{VAST} use synchronization or explicitly write about it.

\begin{itemize}

\item \textit{Athena} acquires data from the combat simulator and the sensors and then it synchronizes them. This data is composed of physiological signals, game events like firing and target hits and haptic device activations. The adopted synchronization technique is not mentioned.

\item \textit{VAST} acquires real-time physiological signals and VR data in a synchronized manner for offline analysis. Markers are used to synchronize the different signals and the virtual environment. 

\end{itemize}

\subsection{Extensions}
This section is not related with similar systems but presents two ideas for extension and improvement of VR experiences that can be useful for the project development.

\begin{itemize}

\item The first idea comes from S. Otsuka and others, who think that physiological signals could be easier acquired if the sensors are embedded in a VR headset \cite{otsuka2017physiological}. They propose a new HMD that contains some sensors: their first prototype consists of a VR headset with an integrated PPG sensor that can measure heart rate. This system was tested and gave positive results, so this kind of devices could be a real and interesting thing for the future of VR and physiological signals research.

\item The second one is about VR and eye tracking. A paper about this is the one by Pradeep Raj and others \cite{kb2017gaze}. It presents the design of a VR-based social communication platform integrated with eye tracking technology. The authors believe that this kind of technology can be useful for presenting and analyze social situations and communication.

\end{itemize}

\section{Design}
After analyzing similar systems with their potentially useful features, now we can introduce the actual design of this thesis project. 
Since there have been problems and changes during the development phase, the general design has been modified during the implementation of the system. 
The development was divided into three prototypes, each adding a new functionality to the system:

\begin{enumerate}
\item real-time visualization of the VR video, the camera video and the signals stream;
\item recording and synchronization of the streams;
\item replay of the streams and insertion of key markers.
\end{enumerate} 

Here are presented all the design prototypes, from the initial idea to the final result.

\subsection{Initial design}

\subsubsection{General architecture}

The system architecture follows the actors involved in the process: the user and the examiner. The participant is the person who takes part in the experiment: he is equipped with a VR headset (HTC Vive), physiological sensors (BiosignalsPlux kit) and a webcam pointing to his face. The communication between these sensors and the examiner's computer is realized through controllers. The examiner component is constituted by the controllers, a recording and synchronization layer that saves the data on the storage and a Java application used to see in real-time what the user is experiencing and to start/stop the recording. 
Concerning the offline mode of the system, it is represented by two additional blocks. The first one deals with the offline part of the application itself, while the other represents the external applications that can be used to analyze data (e.g. MATLAB or Python). This last part is not covered by the project.
Figure 4.1 shows an overall view of the system.

\subsubsection{Controllers}

Regarding the controllers, in the initial design they communicate with the Java application, that executes every task.
In this case the application represents both the real-time and offline Java applications showed in figure 4.1.
Starting from VR, the development platform is Unity and to send the video stream to the Java application we are going to use the \textit{RockVR} plugin, which allows to record a video of what the user sees through the headset and send it to a streaming server in real time. The application is then going to read the stream and play it using the \textit{MediaPlayer} Java class. For the physiological sensors we can use the official \textit{Plux} API, which allows to easily acquire the signals data from Java.
Finally, for the webcam we are going to use the \textit{JavaCV} library, a wrapper of the \textit{OpenCV library}, for visualizing and recording the video stream. For a detailed description of these tools and libraries see section \ref{sec:thirdparty}. The controllers schema is represented in figure 4.2.

\begin{figure}
\centering
\includegraphics[scale=.5, angle=90]{images/general_schema}
\caption{Initial general architecture}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=.57, angle=90]{images/2_controllers2}
\caption{Controllers initial schema}
\end{figure}

\subsection{First prototype}

\subsubsection{Original schema}
The first prototype only concerns the real-time visualization of the VR video, the camera video and the signals stream. Therefore, we can focus on the left side of figure 4.1: we only consider the user, the controllers and the real-time visualization application (figure 4.3). For what concerns the controllers, we are interest only in the real-time tasks, so the diagram is the one in figure 4.4.

\begin{figure}
\centering
\includegraphics[scale=.5]{images/prot1_initial_design}
\caption{First prototype original architecture}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=.5]{images/prot1_controllers_original2}
\caption{First prototype controllers original schema}
\end{figure}

\subsubsection{Implementation changes}
The original diagrams changed when implementing the prototype. The main problem was about the VR video streaming: encoding it, sending it to the streaming server (localhost) and decoding it generated a delay of about 3 seconds. This resulted in an out of sync visualization, since camera and physiological signals were synchronized but VR was not. This behavior was not acceptable. Even if it could have been possible to synchronize the three sources after the recording, it was not possible to see everything in real-time, failing to satisfy the first requirement.

Trying to find a solution to this problem was not easy. Playing the VR video stream from the Java application means to export what happens in Unity and to read it from an external application. The only suitable technique found was the above one. With it not working, the only reasonable solution was to use two screens and split the real-time part of the application in two pieces: one handling camera and signals visualization and one the VR visualization. The former will be handled by the Java application itself on the first screen and the latter by Unity on the second screen (figure 4.5).

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{images/prot1_implementation}
\caption{First prototype architecture implementation}
\end{figure}

As regards the controllers, we can simply get rid of the VR part because it has no more connections with the main application (figure 4.6).

\begin{figure}[h]
\centering
\includegraphics[scale=.51]{images/prot1_controllers_implementation2}
\caption{First prototype controllers implementation}
\end{figure}

\subsection{Second prototype}
The second prototype was supposed to add the recording functionality to the system. The general schema is exactly the same as the one in figure 4.5, but with the addition of the \textit{Record and sync} module and of the storage (figure 4.7). 

\begin{figure}[h]
\centering
\includegraphics[scale=.41]{images/prot2_general}
\caption{Second prototype architecture}
\end{figure}

Since from the first prototype architecture change we can not visualize the VR video in real-time from the Java application, we can not record it either, so we need to do it from Unity itself. This is the reason why there is also a Unity controller in the diagram (figure 4.8): this component is the RockVR plugin, and it will handle the recording of what the user sees and store it on the hard drive. 

\begin{figure}[h]
\centering
\includegraphics[scale=.47]{images/prot2_controllers2}
\caption{Second prototype controllers}
\end{figure}

This new consideration brings the system to be composed of two modules: the first one is a Java application that plays and records the camera and signals streams, the second one is a Unity package that records the VR video.
At the end of the acquisition, the 3 files ‚Äì VR video, camera video and signals data ‚Äì are synchronized (see section \ref{sec:sync}) in order to be replayed and analyzed properly.

\subsection{Third prototype}
The third and last prototype represents the final version of the system. It adds the replay feature to the application, included the option to insert markers on specific moments of the playback. 

Both the architecture and the controllers schemas are similar to the initial idea one, except for the separation of Java and Unity blocks (figure 4.9 and 4.10). 

\begin{figure}[h]
\centering
\includegraphics[scale=.44]{images/prot3_controllers2}
\caption{Third prototype controllers}
\end{figure}

\section{Synchronization}
\label{sec:sync}
Synchronization can be considered as one of the most important parts of this project. The value of the framework is that it gives researchers the possibility to have a ready-to-use environment that allows them to save valuable time, since everything they record can be easily synchronized.
This section gives an overview of the synchronization problem and describes the solution adopted to solve it.

\begin{figure}[p]
\centering
\includegraphics[scale=.5, angle=90]{images/prot3_architecture}
\caption{Third prototype architecture}
\end{figure}

\subsubsection{Synchronization issue}

The camera video and the signals are acquired from the same application. Since the recording starts when the examiner clicks a button, both the sources start being stored approximately at the same time. There should be no relevant delays (i.e. more than one or two tenths of a second), therefore they should be already synchronized. This means that if the implementation is done correctly, the camera video and the signals file do not need further processing in order to be synchronized.

The problem arises when we also consider VR video. As stated above, it is displayed on the Unity window itself and is recorded through a plugin. Just as the other recording, also this one is initiated by the examiner: since the two recordings events are generated by a human actor, there is no way they start at the exact same time. 

We can consider the following as an example of a real usage of the platform. The examiner has already run the application and Unity, configured everything and started seeing the streams in real-time. Now he wants to record the whole session. He first starts recording from the Java application, acquiring signals and camera video, and then switches to the second screen and starts recording from Unity. Even if this two actions are executed very quickly, there will be an unavoidable delay (at least half a second) between the two recordings.
This fact is not acceptable with respect to the framework idea and requirements, thus we need to find a way to really synchronize the files. 
Synchronization means to align the files or to cut them in order to display at the same moment events and data that happened at the same time. The ideal solution would have been to find a way of starting the two recordings simultaneously, without the double intervention of the examiner. This would have required some kind of connection between Java and Unity and was not easy to achieve. The solution adopted to address the problem was another: employ system timestamps.

\subsubsection{System timestamps}

Timestamps are sequences of characters representing a time unit, generally indicating a date and a time. In this case the used timestamps are the \textit{Unix timestamps}, referring to the total number of seconds elapsed since midnight, January 1, 1970 UTC. When retrieved from a computer, this value depends on the underlying operating system. OS in fact measure time with different milliseconds granularity, for example Windows used 55 milliseconds until the 98 version and 10 milliseconds after. 

The general idea about how to use timestamps for synchronization is to associate to each recorded file the timestamp, in milliseconds, of the moment when the recording starts. This gives us the indication about the time when the recordings started and therefore we can compute the difference between the values to get the exact delay between a source an another. The same steps are applied to the end of the recordings. 

An overview on the synchronization phase can be described as follows. Without loss of generality we can assume that the examiner starts (and stops) recording from the Java application and then continues with Unity:

\begin{enumerate}

\item examiner starts the recording from the Java application;

\item current timestamp is associated to signals and camera video files as \textit{initial timestamp};

\item signals and camera video streams begin to be saved on the storage;

\item examiner starts the recording from Unity;

\item current timestamp is associated to VR video file as \textit{initial timestamp};

\item VR video stream begins to be saved on the storage;

\item examiner stops the recording from the Java application;

\item current timestamp is associated to signals and camera video files as \textit{final timestamp};

\item signals and camera video streams stop to be saved on the storage;

\item examiner stops the recording from Unity;

\item current timestamp is associated to VR video file as \textit{final timestamp};

\item VR video stream stops to be saved on the storage;

\item difference between signals (same as camera) and VR initial timestamps is computed (we can name this value ); 

\item difference between signals (same as camera) and VR final timestamps is computed (we can name this value \textit{delayEnd}); 

\item first \textit{delayStart} milliseconds of signals and camera video files are cut;

\item last \textit{delayEnd} milliseconds of VR video files are cut.

\end{enumerate}

This procedure guarantees a good level of synchronization, which basically relies on timestamps accuracy. All the information about source files and timestamps is stored on simple text files. Implementation details about synchronization are reported in section \ref{sec:syncrealization}.


\chapter{Realization}

\section{Synchronization}
\label{sec:syncrealization}


\section{Third parties software}
\label{sec:thirdparty}

\section{Code snippets}

\chapter{Validation}
\section{Proof of concept test}

\subsection{Protocol}

\subsection{Results}

\section{Usability test}

\subsection{Protocol}

\subsection{Results}

\chapter{Conclusions}

\backmatter
\cleardoublepage
\phantomsection % Give this command only if hyperref is loaded
\addcontentsline{toc}{chapter}{\bibname}
\bibliographystyle{ieeetr}
\bibliography{bibliography}
\end{document}